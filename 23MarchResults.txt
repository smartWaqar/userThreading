
#######################		Fiber	##############################################

		cycles	st-fend	st-bend		instructions	branches	b-misses	
		(B)		(B)		(B)				(B)

0,32	3.7		2.8		2.1			0.8(0.2/3.6)	98M			20M			
0,1		12.8	10.1	8.3			1.5(0.1/6.4)	196M		21M
0,8		48		41		37			3.5(0.07/12)	440M		26M



-		c-ref 	c-misses 	l1-load-misses	l1-store-misses		l2 misses


0-32	68T		19T				91T				32T				1.3T	
0,1 	131M	27T				131M			615T			8.9T
0,8		77M		77M				78M				6M				62M



#######################		Delegation	##########################################

		cycles	st-fend	st-bend		instructions	branches	b-misses	
		(B)		(B)		(B)				(B)

0,32	5.6		4		3			2.8(0.5/1.4)	861M		5M						
0,1		12		4.6		4			18(1.5/0.25)	6B			14M
0,8		30		21		20			23(0.8/0.9)		8B			7M



-		c-ref 	c-misses 	l1-load-misses	l1-store-misses		l2 misses


0-32	53T		18T				92T				33T				2.2T
0,1 	60M		29T				79M				27T				3.9T	
0,8		54M		46M				59M				544T			35M


###################################################################################
Change of passing context instead of pointer to the thread 

	Fiber
HT	118,18
SS	392,25
DS  1700s or 2500s (prev 3000s)
2S


################# 27 March ###########################################
######## Fiber after changing position of rep nops ###################

		Fiber
HT		109,4
SS		427,33
DS		2508,344 (1900's for 10x cycles)  ------ 2077, 309 (10 seconds apart)
25		2618, 49								 2505,141  (10 seconds apart) 


DS (10 seconds sleep)
Execution time is 1900 clock cycles
Execution time is 1914 clock cycles
Execution time is 1882 clock cycles
Execution time is 2671 clock cycles
Execution time is 2699 clock cycles
Execution time is 2005 clock cycles
Execution time is 1854 clock cycles
Execution time is 1940 clock cycles
Execution time is 1860 clock cycles
Execution time is 2045 clock cycles





		cycles	st-fend	st-bend		instructions	branches	b-misses	
		(B)		(B)		(B)				(B)

HT
SS
DS



-		c-ref 	c-misses 	l1-load-misses	l1-store-misses		l2 misses


HT
SS
DS


// capture actual temperature, run perf with multiple run option
	
	sensors command
	program19_exp run on 0, 8 
	temperature of the Socket 0: 41 to 45 
	temperature of the Socket 1: 40 to 43
	temperature of the core 0:   39 to 45
	temperature of the core 8: 	 35 to 40


// Figure out L1-dcache-loads 

	nothing useful could be found

// try removing sp_exit_check from fibers

		Fiber
	HT	104, 4
	SS	425, 25
	DS 	2292, 314
	2S

	Execution time is 1916 clock cycles
	Execution time is 2549 clock cycles
	Execution time is 2597 clock cycles
	Execution time is 2534 clock cycles
	Execution time is 2564 clock cycles
	Execution time is 2554 clock cycles
	Execution time is 1909 clock cycles
	Execution time is 2487 clock cycles
	Execution time is 1929 clock cycles
	Execution time is 1884 clock cycles

	L1-Dcache-loads increased from 21B to 24B


// Hardcoding in while loop (came up on my own)

		Fiber
	HT	105, 5
	SS	385, 15
	DS  1871, 120 (one 2219 others smaller)
	2S  2215, 52

	most things are still nearly similar including L1-Dcache-loads 23B

	//using id for changing the bit

		Fiber
	HT  103, 0.9
	SS	387, 26
	DS  1817, 43
	2S




// introduce 1 rep nop in case of else (fiber)

	SS 440, 27
	DS 2488, 424 (1900s or 2900s)






// Introduce 1 rep nop in while loop of delegaton (delegation)

		Delegation (before with 10s)
 	HT	188, 7
	SS	429, 24
	DS  1322, 97
	2S 

		Delegation (after with 10s)
 	HT  187, 5
	SS 	416, 25
	DS 	1305, 57
	2S 	1424, 36


// LLC store misses, dig deep (why 1.5 million difference)

	difference close down

* read it again carefully line by line
// L1-dcache-store-misses, why not the same?

	can't figure out much

// Check for fiber schedular loop, try to bring loads to 1 instead of 4

// check 10 s delay for delegation too



 cpu-cycles
 instructions
 cache-references
 cache-misses
 branch-instructions
 branch-misses
 bus-cycles

 cpu-clock
 task-clock
 page-faults
 minor-faults
 major-faults
 context-switches
 cpu-migrations
 alignment-faults
 emulation-faults

 L1-dcache-loads
 L1-dcache-load-misses
 L1-dcache-stores
 L1-dcache-store-misses
 L1-dcache-prefetches
 L1-dcache-prefetch-misses
 L1-icache-loads
 L1-icache-load-misses
 L1-icache-prefetches
 L1-icache-prefetch-misses
 LLC-loads
 LLC-load-misses
 LLC-stores
 LLC-store-misses

 LLC-prefetch-misses
 dTLB-loads
 dTLB-load-misses
 dTLB-stores
 dTLB-store-misses
 dTLB-prefetches
 dTLB-prefetch-misses
 iTLB-loads
 iTLB-load-misses
 branch-loads
 branch-load-misses                




 cpu-cycles,instructions,cache-references,cache-misses,branch-instructions,branch-misses,bus-cycles

 cpu-clock,task-clock,page-faults,minor-faults,major-faults,context-switches,cpu-migrations,alignment-faults,emulation-faults

 L1-dcache-loads,L1-dcache-load-misses,L1-dcache-stores,L1-dcache-store-misses,L1-dcache-prefetches,L1-dcache-prefetch-misses,L1-icache-loads,L1-icache-load-misses,L1-icache-prefetches,L1-icache-prefetch-misses,LLC-loads,LLC-load-misses,LLC-stores,LLC-store-misses

 LLC-prefetch-misses,dTLB-loads,dTLB-load-misses,dTLB-stores,dTLB-store-misses,dTLB-prefetches,dTLB-prefetch-misses,iTLB-loads,iTLB-load-misses,branch-loads,branch-load-misses




#############################################################3
8,16 Fibers


Execution time is 1949 clock cycles
globalVariable: 10000000
global_var1: 0
global_var2: 0

 Performance counter stats for './program19_exp':

    43,051,945,367      cpu-cycles                #    2.429 GHz                      (19.19%)
    42,136,545,168      instructions              #    0.98  insn per cycle                                              (23.07%)
        77,151,354      cache-references          #    4.352 M/sec                    (23.12%)
        73,142,085      cache-misses              #   94.803 % of all cache refs      (23.16%)
    12,525,460,174      branch-instructions       #  706.579 M/sec                    (23.21%)
        27,433,760      branch-misses             #    0.22% of all branches          (23.25%)
     1,770,076,959      bus-cycles                #   99.853 M/sec                    (15.51%)
      17726.909612      cpu-clock (msec)          #    2.000 CPUs utilized          
      17726.911334      task-clock (msec)         #    2.000 CPUs utilized          
               119      page-faults               #    0.007 K/sec                  
               119      minor-faults              #    0.007 K/sec                  
                 0      major-faults              #    0.000 K/sec                  
                21      context-switches          #    0.001 K/sec                  
                 2      cpu-migrations            #    0.000 K/sec                  
                 0      alignment-faults          #    0.000 K/sec                  
                 0      emulation-faults          #    0.000 K/sec                  
    21,026,969,546      L1-dcache-loads           # 1186.161 M/sec                    (15.28%)
        79,228,286      L1-dcache-load-misses     #    0.38% of all L1-dcache hits    (7.67%)
       184,631,133      L1-dcache-stores          #   10.415 M/sec                    (7.67%)
         3,909,223      L1-dcache-store-misses    #    0.221 M/sec                    (7.67%)
   <not supported>      L1-dcache-prefetches                                        
        32,949,846      L1-dcache-prefetch-misses #    1.859 M/sec                    (7.67%)
   <not supported>      L1-icache-loads                                             
           457,553      L1-icache-load-misses                                         (11.51%)
   <not supported>      L1-icache-prefetches                                        
   <not supported>      L1-icache-prefetch-misses                                   
        66,641,324      LLC-loads                 #    3.759 M/sec                    (15.34%)
        62,735,012      LLC-load-misses           #   94.14% of all LL-cache hits     (15.34%)
        10,628,073      LLC-stores                #    0.600 M/sec                    (7.67%)
        10,631,235      LLC-store-misses          #    0.600 M/sec                    (7.67%)
         5,104,920      LLC-prefetch-misses       #    0.288 M/sec                    (7.67%)
    21,013,323,771      dTLB-loads                # 1185.391 M/sec                    (7.67%)
        20,087,601      dTLB-load-misses          #    0.10% of all dTLB cache hits   (7.67%)
       184,755,107      dTLB-stores               #   10.422 M/sec                    (7.67%)
            11,484      dTLB-store-misses         #    0.648 K/sec                    (7.67%)
   <not supported>      dTLB-prefetches                                             
   <not supported>      dTLB-prefetch-misses                                        
             1,317      iTLB-loads                #    0.074 K/sec                    (7.67%)
             3,545      iTLB-load-misses          #  269.17% of all iTLB cache hits   (11.51%)
    12,520,296,262      branch-loads              #  706.288 M/sec                    (15.34%)
        27,372,591      branch-load-misses        #    1.544 M/sec                    (15.34%)

       8.865149283 seconds time elapsed




8, 16 Delegation

Execution time is 1293 clock cycles
globalCounter 10000000 

 Performance counter stats for './delg':

    27,128,619,168      cpu-cycles                #    2.306 GHz                      (19.04%)
    16,958,572,727      instructions              #    0.63  insn per cycle                                              (22.91%)
        54,861,199      cache-references          #    4.664 M/sec                    (22.98%)
        45,752,915      cache-misses              #   83.398 % of all cache refs      (23.05%)
     5,567,437,269      branch-instructions       #  473.288 M/sec                    (23.12%)
         7,745,862      branch-misses             #    0.14% of all branches          (23.19%)
     1,174,521,853      bus-cycles                #   99.846 M/sec                    (15.51%)
      11763.305752      cpu-clock (msec)          #    1.999 CPUs utilized          
      11763.309648      task-clock (msec)        3,909,223      L1-dcache-store-misses    #    0.221 M/sec 3,909,223      L1-dcache-store-misses    #    0.221 M/sec  #    1.999 CPUs utilized          
                88      page-faults               #    0.007 K/sec                  
                88      minor-faults              #    0.007 K/sec                  
                 0      major-faults              #    0.000 K/sec                  
                14      context-switches          #    0.001 K/sec                  
                 3      cpu-migrations            #    0.000 K/sec                  
                 0      alignment-faults          #    0.000 K/sec                  
                 0      emulation-faults          #    0.000 K/sec                  
     5,592,904,108      L1-dcache-loads           #  475.453 M/sec                    (15.23%)
        59,171,773      L1-dcache-load-misses     #    1.06% of all L1-dcache hits    (7.75%)
        52,977,029      L1-dcache-stores          #    4.504 M/sec                    (7.75%)
           140,822      L1-dcache-store-misses    #    0.012 M/sec                    (7.75%)
   <not supported>      L1-dcache-prefetches                                        
        42,906,438      L1-dcache-prefetch-misses #    3.647 M/sec                    (7.75%)
   <not supported>      L1-icache-loads                                             
           282,661      L1-icache-load-misses                                         (11.63%)
   <not supported>      L1-icache-prefetches                                        
   <not supported>      L1-icache-prefetch-misses                                   
        42,827,790      LLC-loads                 #    3.641 M/sec                    (15.51%)
        33,820,826      LLC-load-misses           #   78.97% of all LL-cache hits     (15.51%)
        11,935,321      LLC-stores                #    1.015 M/sec                    (7.72%)
        11,919,440      LLC-store-misses          #    1.013 M/sec                    (7.65%)
         2,331,252      LLC-prefetch-misses       #    0.198 M/sec                    (7.62%)
     5,592,935,562      dTLB-loads                #  475.456 M/sec                    (7.62%)
            36,241      dTLB-load-misses          #    0.00% of all dTLB cache hits   (7.62%)
        53,048,077      dTLB-stores               #    4.510 M/sec                    (7.62%)
             5,934      dTLB-store-misses         #    0.504 K/sec                    (7.62%)
   <not supported>      dTLB-prefetches                                             
   <not supported>      dTLB-prefetch-misses                                        
               460      iTLB-loads                #    0.039 K/sec                    (7.62%)
               228      iTLB-load-misses          #   49.57% of all iTLB cache hits   (11.42%)
     5,561,456,656      branch-loads              #  472.780 M/sec                    (15.23%)
         7,526,395      branch-load-misses        #    0.640 M/sec                    (15.23%)

       5.885141760 seconds time elapsed







##############################################
April 14 Meeting Shifted to April 21
##############################################

* (Run both with 10 seconds pause and without it. Also brainstorm other ideas causing the 
slow down. Google PowerState)

	Without 10 sec
	sensors command
	program20_exp run on 0, 8 
	temperature of the Socket 0: 42 to 45 
	temperature of the Socket 1: 42 to 46
	temperature of the core 0:   40 to 45
	temperature of the core 8: 	 36 to 43


	With 10 sec
	sensors command
	program20_exp run on 0, 8 
	temperature of the Socket 0: 42 to 44 
	temperature of the Socket 1: 42 to 43
	temperature of the core 0:   39 to 44
	temperature of the core 8: 	 37 to 41

	In first runs the temperature didn't increased that much


	Execution time is 2827 clock cycles
	Execution time is 2499 clock cycles
	Execution time is 1947 clock cycles
	Execution time is 2248 clock cycles
	Execution time is 1932 clock cycles
	Execution time is 2440 clock cycles
	Execution time is 2800 clock cycles
	Execution time is 1858 clock cycles
	Execution time is 2425 clock cycles
	Execution time is 1887 clock cycles




* Send email for Sublime Subscription
* Check again for frequency scaling

Run on 0, 8

1 run not bound:
	 2.360 GHz (1948 cycles) 
	 2.584     (1893)
	 2.432		(2947)
	 2.552		(1924)

	 num of cycles and instruction got increased

10 run not bound:
	Either 1900s or 2900s

	Freq: 2.463 GHz (+- 7.70 %)

	2970
	1895
	2909
	1994
	1913
	2861
	1928
	2996
	1890
	2953

10 run Bound to 0:
	Freq 2.483 GHz (+- 6.42)

10 run Bound to 8:
	Freq 2.438 Ghz (+- 7.83)


* Try the experiment on other machines as well
	10s gap

	Lines: (Not sure about the architechture)

	HT(0, 32)	2005, 94		
	SS(0,1) 	251, 8.95
	DS(0,8)		1809, 119
	DS(0,8)		1859, 79  (Without 10s)

	Nodes: 

	HT(0, 32)	1724, 85	(Actuall 2 Hops away)
	SS(0,1)		539, 46
	DS(0,8)		493, 22
	DS(0,8)

	HT(0, 64)	89.4, 2.0
	DS(0, 16)	1334, 174





* Measure the other temps like Memory or Memory controller or Bus temp (idk whether there 
are sensors there). Search for off core temperature sensors. 

-- Not much sesnors abroad



###############################################

# Repeat the experiment on Quads as well (1 Machine out of the 4 is enough)
# Repeat the other experiments (delegation/locks) on Nodes and Quads as well

# Any user fiber library (e.g libfiber start with that) check for performance of fiber migrations

# Explore Rust (write some small programs and try doing the delegation in rust(not really)), Noman and Ben are deep in Rust 

-> Picking up Rust isn't that easy. Easily a month to pickup, Syntax isn't a big deal but Model/core-concepts is: borrow-checker, reference lifetime

-> Talk to Ben and Noman about how they feel about Rust


###################################################
April 27


# Repeat the experiment on Quads as well (1 Machine out of the 4 is enough)
# Repeat the other experiments (delegation/locks) on Nodes and Quads as well

Experiments with 10s wait


Nodes: 

		Fiber (From Past week)
	HT(0, 64)	89.4, 2.0
	SS(0,1)		539, 46
	DS(0, 16)	1334, 174
	2S(0, 32)	1724, 85


		Delegation
	HT(0, 64)	150, 0.4
	SS(0,1) 	463, 39
	DS(0,16)	986, 65
	2S(0, 32)   1151, 124

		Locks (Spin)
	HT(0,64)	58, 4
	SS(0,1) 	172, 36
	DS(0,16)	354, 37
	2S(0, 32)	471, 40

		Locks (Mutex)
	HT(0,64)	70, 0	
	SS(0,1)		313, 17
	DS(0,16)	316, 22
	2S(0,32)	321, 31



54, 2 

Quads 1: 

		Fiber
	HT(0,28)	587, 1.3  (Repeated Twice, Similar Result)	  
	SS(0,1)		747, 8.1 
	DS(0,14)	1148, 88
	2S

		Delegation
	HT(0,28)	1120, 1.1
	SS(0,1)		1500, 35 	
	DS(0,14)    2042, 84
	2S

		Locks (Spin)
	HT(0,28)	508, 1
	SS(0,1)     571, 3.6
	DS(0,14)	695, 22
	2S

		Locks (Mutex)
	HT(0,28)	527, 0.5
	SS(0,1) 	698, 20
	DS(0,14)	802, 28
	2S



# Any user fiber library (e.g libfiber start with that) check for performance of fiber migrations

-> Boost: A fiber c


###########################################################

# Investigate by changing repnops for quads experiment
# Try passing the core number as a parameter
# Try ticket locks for preventing reacusiation / and measure how often re-acquisition occurs
-> add another counter to check for re-acquisitions 

# With and without rep nops (fiber and delegation) 

get-opt (function for command line arguments) there are other as well -- google command line 
params in c

#Use Makefile (compile time arguments -D, by the use of #ifdef change whethere to use repnops or not)


########################################################



------- Quad 3 Experiment -------
Without Sleep
Without Repnops

Fiber
HT(0, 28) Mean:111.8  SD: 0.4
SS(0, 1)  Mean:719.5  SD: 19.9261
DS(0, 14) Mean:1541  SD: 69.0912
---------------------------------------
delegation_exp.c
HT(0, 28) Mean:86.3  SD: 0.781025
SS(0, 1)  Mean:471  SD: 20.283
DS(0, 14) Mean:548.1  SD: 12.5096
----------------------------------------
locks_exp.c with spinlocks
HT(0, 28) Mean:24  SD: 0
SS(0, 1)  Mean:102.2  SD: 4.21426
DS(0, 14) Mean:281.5  SD: 29.588

######################################

With Repnops

Fiber
HT(0, 28) Mean:536.8  SD: 1.6
SS(0, 1)  Mean:698.1  SD: 7.51598
DS(0, 14) Mean:1146  SD: 53.4359
---------------------------------------
delegation_exp.c
HT(0, 28) Mean:1120.7  SD: 2.49199
SS(0, 1)  Mean:1465  SD: 37.2586
DS(0, 14) Mean:1995.6  SD: 107.72
----------------------------------------
locks_exp.c with spinlocks
HT(0, 28) Mean:507.1  SD: 0.830662
SS(0, 1)  Mean:571.8  SD: 2.44131
DS(0, 14) Mean:686.5  SD: 26.5603

########################################################

Special Counter for spinlocks re-aquisition

Without Repnops 
locks_exp2.c with spinlocks

HT(0, 28) 4672794
SS(0, 1)  4850267
DS(0, 14) 4475495

With Repnops

HT(0, 28) 2087
SS(0, 1)  1902
DS(0, 14) 53953








#######################################################
(Meeting Saturday May 16, For Next Meeting on May 19)
# ticket locks (I can a)
# Add to your script a gnu plot which makes a bar chart
set the file to the bar chart
#Repeat the experiment on other machine (e.g. nodes)

#####################################################
Summer goals

Green sleep branch in rust branches link shared by the 

Berrel Fish Operating System EPFL
######################################################



------- Nodes Experiment -------
Without Sleep
Without Repnops

program20_exp.c 

HT(0, 64) Mean:108.5  SD: 3.23265
SS(0, 1)  Mean:562.9  SD: 28.6965
DS(0, 16) Mean:1742.5  SD: 198.638
---------------------------------------
delegation_exp.c

HT(0, 64) Mean:87.9  SD: 0.538516
SS(0, 1)  Mean:448.7  SD: 54.2937
DS(0, 16) Mean:770.7  SD: 128.322
----------------------------------------
locks_exp.c with spinlocks

HT(0, 64) Mean:41.7  SD: 1.48661
SS(0, 1)  Mean:71.8  SD: 11.4
DS(0, 16) Mean:78.1  SD: 9.37497

######################################
With Repnops

program20_exp.c 

HT(0, 64) Mean:89.1  SD: 3.83275
SS(0, 1)  Mean:542  SD: 37.9895
DS(0, 16) Mean:1618.1  SD: 155.775
---------------------------------------
delegation_exp.c

HT(0, 64) Mean:173.9  SD: 13.5311
SS(0, 1)  Mean:435.7  SD: 38.6783
DS(0, 16) Mean:938.3  SD: 140.526
----------------------------------------
locks_exp.c with spinlocks

HT(0, 64) Mean:57.1  SD: 0.3
SS(0, 1)  Mean:191.2  SD: 20.6485
DS(0, 16) Mean:354.7  SD: 23.4011


###############################################

Time with Ticket Locks

------- Nodes Experiment -------
Without Sleep
----------------------------------------
Without repnops
locks_exp2.c with ticketLocks

HT(0, 64) Mean:66  SD: 4.64758
SS(0, 1)  Mean:198  SD: 27.6659
DS(0, 16) Mean:367.1  SD: 43.3208
----------------------------------------
With repnops
locks_exp2.c with ticketLocks

HT(0, 64) Mean:57  SD: 0
SS(0, 1)  Mean:192.5  SD: 17.8059
DS(0, 16) Mean:330.1  SD: 39.5056


############################################


Special Counter for ticketlocks re-aquisition

Without Repnops 
locks_exp2.c with ticketlocks

HT(0, 64) 150-2000
SS(0, 1)  300-1000
DS(0, 16) 90000-1475000

With Repnops

HT(0, 64) 800-2000
SS(0, 1)  2000-7000
DS(0, 16) 1568000-3436000

##########################################
Confirmed TicketLock

------- Nodes Experiment -------
Without Sleep
----------------------------------------
Without repnops
locks_exp2.c with ticketlocks


HT(0, 64) Mean:73.3  SD: 0.458258
SS(0, 1)  Mean:215.2  SD: 32.4586
DS(0, 16) Mean:233.3  SD: 82.0074
----------------------------------------
With repnops
locks_exp2.c with ticketlocks

HT(0, 64) Mean:66.1  SD: 4.52659
SS(0, 1)  Mean:277.3  SD: 35.2308
DS(0, 16) Mean:333.1  SD: 101.206


##########################################

Bar chart ???

HT, SS, 


##################################

Nodes will be busy, try quads

# Start using bar chart

#Measure where we are spending time, use perf c2c (not time, cache misses) and with perf record
-> Investigage for the round trip latency for fibers

# Lookup __rdtsc (compiler intrinsic) for measuring time instead of what I am already doing

# Move thread pinning in the threads
# Measure time after the entering and leaving the barrier

# Rust repository brach: green_sleep 
Make a new test case fiber migration (most test cases will be in lib.rs)
normal delegation with a function calling a function
wrapping the varialbe in trust

-> End goal: migrate the function to the server and the function keep on running

-> Print migrate -> Check which client -> Check which fiber -> Add that to the run 


#################################


Nodes will be busy, try quads

# Start using bar chart (Done)

#Measure where we are spending time, use perf c2c (not time, cache misses) and with perf record
-> Investigage for the round trip latency for fibers (Done)


c2c HT Fiber
17 cpu/mem-loads, ldlat=30/P (Cannot decipher)
4k cpu/mems-stores/P
	44 Scheduler -> 86% mov ecx, 0x38(%rax)
	42 pfc -> add to global variable
	13 Context Switch


c2c SS Fiber
23k cpu/mem-loads, ldlat=30/P
	86 Schedular -> 100% mov 0x38(%rax), %ecx
	7  contextSwitch
28k cpu/mems-stores/P
	53 Schedular -> call to context Switch
	40 pfc -> Additon to global variable
	2 contextSwitch


c2c DS Fiber
55k cpu/mem-loads, ldlat=30/P
	90 Schedular -> 100% mov 0x38(%rax), %ecx
	9  contextSwtich
62k cpu/mems-stores/P
	49 Scheudlar -> call to context Switch
	38 pfc -> Additon to global variable


Comment: All the store/loads are at the same location in these experiments, I guess some of which is expected and can't trivially be optimized 

# Lookup __rdtsc (compiler intrinsic) for measuring time instead of what I am already doing
	I have updated the fiber but not others yet


# Move thread pinning in the threads (Done)
# Measure time after the entering and leaving the barrier (Done)
	doesn't make any difference

# Rust repository brach: green_sleep 
Make a new test case fiber migration (most test cases will be in lib.rs)
normal delegation with a function calling a function
wrapping the varialbe in trust

-> End goal: migrate the function to the server and the function keep on running

-> Print migrate -> Check which client -> Check which fiber -> Add that to the run 

	Problem with this task, some features used in the program are depreacted, I tried to correct them but couldn't succeedd. End up with a segmentation fault





####################################################


#Update the bar grpah with proper titles etc
#Use two global vairable configuration for fibers (no need for a cache miss) - measure c2c again + also regenerate the plot (Done)


fiber 107 490.7 1104.1
delegation 86.9 478.9 541
locks 24 103.6 285.1
fiberRN 534.7 652.2 769.3
delegationRN 1120.9 1477.1 1950.6
locksRN 507.1 570.7 702

HT 
18 cpu/mem-loads, ldlat=30/P
4k cpu/mems-stores/P 
	47% Scheduler -> 12% callq contextSwitch, 88% mov %ecx, 0x38(%rax)
	42% pfc -> 50% mov %rax, globalvar1, 50% %rdx, globalvar2
	11% contextSwitch


SS
8k cpu/mem-loads, ldlat=30/P
	74% contextSwtich
	26% Scheduler -> 100% mov 0x38(%rax),%eax
18k cpu/mems-stores/P 
	49% Scheduler -> Same as above
	42% pfc -> Same as above
	7% conextSwitch


DS
39K cpu/mem-loads, ldlat=30/P
	88% Scheduler -> Same as above
	12% ContextSwitch
43K cpu/mems-stores/P 
	44% Scheduler
	40% pfc
	8% contextSwitch


I have tried changing alignment of global varialbes from 128 to 64 and long long to long 
but it didn't work
When I changed the global variables to non volatile there is an improvement in performance
(39K, 43K) to (35K, 38K) and also a bit in time as well. Now the pfc 50% are not on mov but on addq of global variables



#Lookup the documentation for the mem-loads and the mem-stores in perf c2c 
c2c HT Fiber
17 cpu/mem-loads, ldlat=30/P (Cannot decipher)
4k cpu/mems-stores/P
	44 Scheduler -> 86% mov ecx, 0x38(%rax)
	42 pfc -> add to global variable
	13 Context Switch

-> Sharing configuraition for the cache betwen hyper threads?

share mode vs adaptive mode on L1 cache intel


# Investigate why there are cache misses in case of HT
# Investigate why so few cahce misses in case of SS/DS, check how perf do the c2c maybe its because of how it samples

Random Sampling  / shared mode

#Check ben for how to run fiber green sleep

#Stop the threads in the middle of execution in check whether their rsps are in different 
2Mb segments

$rsp are the same when checked for both threads
print UserThreadingVec[0].current_thread.context
$16 = {rsp = 140737345503216, r15 = 0, r14 = 0, r13 = 0, r12 = 93824994337280, rbx = 9999998, 
  rbp = 93824994337280, mxcsr = 1}






########################################################333

#Update the bar grpah with proper titles etc (Done)

#Use two global vairable configuration for fibers (no need for a cache miss) - measure c2c again + also regenerate the plot

#Lookup the documentation for the mem-loads and the mem-stores in perf c2c 
c2c HT Fiber
17 cpu/mem-loads, ldlat=30/P (Cannot decipher)
4k cpu/mems-stores/P
	44 Scheduler -> 86% mov ecx, 0x38(%rax)
	42 pfc -> add to global variable
	13 Context Switch

-> Sharing configuraition for the cache betwen hyper threads?

# Investigate why there are cache misses in case of HT
# Investigate why so few cahce misses in case of SS/DS, check how perf do the c2c maybe its because of how it samples

#Check ben for how to run fiber green sleep

#Stop the threads in the middle of execution in check whether their rsps are in different 
2Mb segments





####################################

#Just for fiber migration try with different number of rep nops, try both lower and higher for what I have rn (Done)

Fiber Experiments Last one is with 10 RepNops without Any in Else Condition

RN0 	107 488.7 1093.2
RN1 	135.1 395.7 979.6
RN5 	283.7 404.6 840.5
RN10	534.3 650.5 785.1
RN20	1038.9 1153.7 1249.6
RN10WElse 535.2 652.3 921.2



#Setup ssh config for password less and portless logins (Tried the procedure on differnet
blogs i.e. generate keys and copy id but didn't work at the end still requires password)
#Why same on HT, SS for fiber and delegation but such different 


#On Using AVX512

Frequency decreasing from 2.6 to 2.2
fiber 129.7 565.7 1193.8 (Store right before setting the bit)

fiber 98.2 501.8 1132 (Not Using intrinsics)



-> Flipping bit in assembly
fiber 80.5 494.1 1124

+ with only one repnop in else
fiberRN 85.8 384.4 1012.7



cargo test one_inc (didn't compile)
src/lib.rs


################
Try flipping in assembly with RN10
fiberRN 534.9 659.3 793.9

# Make assembly inline and I will see a large increase in performance

Didn't even compile without -O3 flag:
	failed with "bp cannot be used in asm here"

#########################

#make inline contextSwithch function and use it the two locations
# Email + Sherice Stwered about sublime purchase


##########################

fiber 111.7 343.9 735.8

1 Repnop means 1 in the while and 1 in else. All repnops experiments are with 1 in else

DS 7,28 Quads 3:

0 RepNop: 741.3, 65
1 RepNop: 711.3, 81
2 RepNop: 669.4, 74
3 RepNop: 643.7, 106
4 RepNop: 688.2, 90
5 RepNop: 663.7, 84


Quads 2 Regular:
fiber 235.1 390.7 566.7
2nd time: 
fiber 111.4 357.4 739.3

1 : Mean:661.4  SD: 98.4715
2: Mean:715.5  SD: 96.9363
3: Mean:752.7  SD: 122.165  (521)

Again 0 Repnops: Mean:672.8  SD: 62.5361

Quads 4:
fiber 111.5 361.2 747.4





Details: 
0 RepNop
Execution time is 671 clock cycles
Execution time is 858 clock cycles
Execution time is 769 clock cycles
Execution time is 689 clock cycles
Execution time is 825 clock cycles
Execution time is 808 clock cycles
Execution time is 669 clock cycles
Execution time is 719 clock cycles
Execution time is 709 clock cycles
Execution time is 696 clock cycles

1 RepNop
Execution time is 763 clock cycles
Execution time is 801 clock cycles
Execution time is 638 clock cycles
Execution time is 683 clock cycles
Execution time is 563 clock cycles
Execution time is 755 clock cycles
Execution time is 802 clock cycles
Execution time is 625 clock cycles
Execution time is 802 clock cycles
Execution time is 681 clock cycles

2 RepNop
Execution time is 530 clock cycles
Execution time is 638 clock cycles
Execution time is 601 clock cycles
Execution time is 676 clock cycles
Execution time is 687 clock cycles
Execution time is 796 clock cycles
Execution time is 704 clock cycles
Execution time is 667 clock cycles
Execution time is 771 clock cycles
Execution time is 624 clock cycles

3 RepNops
Execution time is 570 clock cycles
Execution time is 696 clock cycles
Execution time is 582 clock cycles
Execution time is 603 clock cycles
Execution time is 516 clock cycles
Execution time is 626 clock cycles
Execution time is 732 clock cycles
Execution time is 909 clock cycles
Execution time is 593 clock cycles
Execution time is 610 clock cycles

Quads2 3 Repnops:
Execution time is 521 clock cycles
Execution time is 931 clock cycles
Execution time is 894 clock cycles
Execution time is 859 clock cycles
Execution time is 644 clock cycles
Execution time is 835 clock cycles
Execution time is 705 clock cycles
Execution time is 646 clock cycles
Execution time is 737 clock cycles
Execution time is 755 clock cycles




#################################

June 23 Meeting tasks

#Measure today's code performance

#128 bit algined context and non clobberd but stored regisgters

# Save half of registers and clobber other half (assumption the compiler will not used the clobbered registers)

# Yield should also be inlined (always inline) -> move Yield to .h


# profile about cache misses and read assembly before and after entering the block

###################################


#Measure today's code performance

fiber 109.9 455.7 859.5

6 RepNops: Mean:833.1  SD: 101.655
7 RepNops: Mean:718.6  SD: 57.651
8 RepNops: Mean:810.1  SD: 53.4873

-> Tweak varialbes a bit:
fiber 116.7 446.2 793.5

Quads3 0 14 DS:
Mean:887  SD: 129.763
Execution time is 996 clock cycles
Execution time is 875 clock cycles
Execution time is 660 clock cycles
Execution time is 925 clock cycles
Execution time is 981 clock cycles
Execution time is 912 clock cycles
Execution time is 1063 clock cycles
Execution time is 982 clock cycles
Execution time is 669 clock cycles
Execution time is 807 clock cycles


#128 bit algined context and non clobberd but stored regisgters

-> If rcx is not clobbered only one thread works, so measured with rcx 
fiber 106.4 501.2 838.9

later when near fix: segmentation fault


# Save half of registers and clobber other half (assumption the compiler will not used the clobbered registers)

fiber 113.7 459.1 814



# Cache misses:

Orginial Configuration (All clobbered registers):
(0, 1) => 119 M Ref, 0.009% Misses
(0,14) => 59 M Ref, 99.9 % Misses

Half Non-clobbered register configuration:
(0, 1) => 122M Ref, 0.006% Misses
(0,14) =>  67M Ref, 99.9% Misses

Half Non-clobbered register configuration:
(0, 1) => 142M Ref, 0.006% Misses
(0,14) =>  88M Ref, 99.9% Misses



############################################

# Do the repnops from the command line 
-D RepNops=?
#Figure the variations in result
# Make the wait long in fiber to minimize cache-misses: try to achieve around 10 to 20 million
#Plot the performance vs Cache misses (Scatter plot) 
# Rust side explore 

##############################################

Repnops Performance

Manually With Hand
0 => 758, 140
1 => 601, 41
2 => 597, 47

0 again: 
Mean:806.3  SD: 166.584
Mean:656.5  SD: 102.278

1 again:
Mean:571.6  SD: 20.9771

2 again:
Mean:667.7  SD: 61.1932


Using exp_script6.sh
Mean:615.3  SD: 53.874
Mean:626.2  SD: 79.6276
Mean:666.7  SD: 90.4677
Mean:599.5  SD: 53.2339
Mean:593.3  SD: 53.3068
Mean:687  SD: 89.9789
Mean:650.1  SD: 41.1933
Mean:623  SD: 41.754
Mean:629.3  SD: 69.0305
Mean:661.3  SD: 39.3219
Mean:596.4  SD: 63.3959


DS (0 to 10 Again after modifiction in script file)
Mean:765.7  SD: 135.573
Mean:574.6  SD: 15.519
Mean:615  SD: 38.7298
Mean:600  SD: 74.8839
Mean:595.7  SD: 49.1855
Mean:583.2  SD: 44.2624
Mean:658.2  SD: 41.2984
Mean:576.7  SD: 22.8125
Mean:616.6  SD: 14.8135
Mean:669.7  SD: 12.6732
Mean:702.5  SD: 6.0208



# Do the repnops from the command line 
-D RepNops=?  (Done)

#Figure the variations in result

perf record cache-misses
49% mpthread1
33% mpthread2
12% target769
5%  pfc

Similar results of -e cache-references and cache-misses

# Make the wait long in fiber to minimize cache-misses: try to achieve around 10 to 20 million

#Plot the performance vs Cache misses (Scatter plot) (Done)


DS without Repnops
55M 555
63M 736
58M 641
68M 734
81M 920
58M 613
83M 819
80M 840
80M 858
57M 584


Tried adding a fixed number of i in else

No Repnops
Mean:675.5  SD: 86.0038
1 
Mean:615.8  SD: 77.2487
2
Mean:618.9  SD: 55.3651

# Rust side explore 


##################################################

1 RepNop:
Mean:697.9  SD: 32.5959
2 
Mean:727  SD: 76.2955
3
Mean:656.2  SD: 71.8704


RepNops [1 to 10]

Mean:704.4  SD: 22.9878
Mean:672.5  SD: 48.9576
Mean:687.5  SD: 83.385
Mean:669.3  SD: 77.3822
Mean:721.1  SD: 67.9624
Mean:700.6  SD: 58.2584
Mean:664.4  SD: 59.3704
Mean:704.3  SD: 65.0877
Mean:792.8  SD: 44.9351
Mean:773.2  SD: 25.107

   
-> Consistency Problem
-> Unexpected Cache-miss problem



##################################################

read on git workflow, how people actually use git

Chapter 8.2 Memory ordering. Intel Developer Manual 

Try some random stuff 
Read on memory order
Try putting memory fences on different locations


####################################################

Meeting skipped for the next week due to Eye Surgery

####################################################
Work for July 28 Meeting

-> asm volatile("": : :"memory");

0,14
Mean:573.7  SD: 31.9501

0,14 With RepNops
Mean:562  SD: 50.8626


RepNops 1 to 10:

Mean:562  SD: 55.7225
Mean:565.8  SD: 76.1062
Mean:538.1  SD: 43.4752
Mean:538.5  SD: 54.3751
Mean:544.7  SD: 56.1303
Mean:562.1  SD: 21.0972
Mean:575.4  SD: 49.512
Mean:549.3  SD: 20.2141
Mean:585.4  SD: 5.48088
Mean:638  SD: 4.38178



-> Using sync_synchornize()

Mean:564.4  SD: 52.5209


3 RepNOPS Mean:544.7  SD: 32.0064


RepNops 1 to 10:

Mean:539.2  SD: 51.0525
Mean:531.3  SD: 32.7018
Mean:574.9  SD: 65.3735
Mean:556.2  SD: 36.174
Mean:605  SD: 45.6092
Mean:629.8  SD: 16.4
Mean:693.9  SD: 40.9572
Mean:740.6  SD: 9.86103
Mean:795.6  SD: 57.8864
Mean:846.6  SD: 36.1613




-> Using volatile int i (pfc loop integer) Solved the problem but the performance get nearly twice worse
Mean:1023.1  SD: 89.9694




Professors's Suggestion on slack:

1. Instead of a memroy fence use a for loop with no body to see whehter we get a different
result

Didn't work. I guess compiler removes the loop as dead code

2. Check if flag inconsistency is the culprit: use sequence number instead of true/false 0/1

Didn't work. Same problem. 
Run the code with pfc printf same problem



-> Notes: Memory Barrier Stack overflow

asm volatile("": : :"memory") vs GCC builtin __sync_synchronize
One is a compiler barrier other is actual hardware barrier. 

Types of Memory Barriers
As mentioned above, both compilers and processors can optimize the execution of instructions in a way that necessitates the use of a memory barrier. A memory barrier that affects both the compiler and the processor is a hardware memory barrier, and a memory barrier that only affects the compiler is a software memory barrier.

In addition to hardware and software memory barriers, a memory barrier can be restricted to memory reads, memory writes, or both. A memory barrier that affects both reads and writes is a full memory barrier.

There is also a class of memory barrier that is specific to multi-processor environments. The name of these memory barriers are prefixed with "smp". On a multi-processor system, these barriers are hardware memory barriers and on uni-processor systems, they are software memory barriers.

The barrier() macro is the only software memory barrier, and it is a full memory barrier. All other memory barriers in the Linux kernel are hardware barriers. A hardware memory barrier is an implied software barrier.



#######################################33

Check assembly and use diff: try to understand why the memory clobber fix the problem

See how the assembly changes with mfence and why difference fixes the problem


Remember what you set last time and xorq and 

Use 1 bit flag

Stride prefetching ???

#Change using userthreading away from global


explicit prefetch command and use that for one of the addresses for the later part of the contxt

Other thing interleave the register/ reorder the registers 8 bits from 1 cache line 8 bit from the other 


##############################################


#Check assembly and use diff: try to understand why the memory clobber fix the problem

#See how the assembly changes with mfence and why difference fixes the problem


Checked the problem seems to be that the compiler put variable s and t outside the loop in one case


#Remember what you set last time and xorq and 

#Use 1 bit flag

Done
Mean:594.1  SD: 53.8562


#Stride prefetching ???

#Change using userthreading away from global

Doesn't seem possible due to the global nature of use


explicit prefetch command and use that for one of the addresses for the later part of the contxt

Other thing interleave the register/ reorder the registers 8 bits from 1 cache line 8 bit from the other 


Interleaving all:
Mean:752.3  SD: 184.795

Interleave on loading:
Mean:611.7  SD: 103.516

Execution time is 588 clock cycles
Execution time is 905 clock cycles
Execution time is 554 clock cycles
Execution time is 556 clock cycles
Execution time is 556 clock cycles
Execution time is 571 clock cycles
Execution time is 617 clock cycles
Execution time is 545 clock cycles
Execution time is 563 clock cycles
Execution time is 662 clock cycles


Mean:618.2  SD: 72.6083

Execution time is 551 clock cycles
Execution time is 549 clock cycles
Execution time is 555 clock cycles
Execution time is 691 clock cycles
Execution time is 559 clock cycles
Execution time is 550 clock cycles
Execution time is 597 clock cycles
Execution time is 707 clock cycles
Execution time is 701 clock cycles
Execution time is 722 clock cycles


Mean:582.2  SD: 71.8621
REPNOP 1:

Mean:561.3  SD: 46.1174
RepNOP 2:
Mean:605.2  SD: 92.8534



-> Builtin prefetch after the above (only in Scheduler)

Mean:608.4  SD: 75.5542




#######################################################

# Re-investigage the problem

# Put lots of local varialbes > 15-16 and give them unique values and check the values again after moving to the other core

 One way to fix is that inside the if there is a printf which depends on a global volatile variable

 Reference the video recording for solution


# Percise mesurement of cycles on quads with perf -e cycles:ppp

	perf list | less 
	/Precise



# Pointer array and malloc (Pointer array and numa alloc as well )instead of global array 

# Fix the bug and profile again and check if the problem remains



###########################################################


Performance with the right builtin prefetch Mean:605.2  SD: 85.6712
Perforamnce without prefetch Mean:571.4  SD: 41.5432


-> Bug fixed with a printf before the loop

Performance: Mean:588.6  SD: 75.1095

-> Reason for the bug ??? 
	why is printf solving the problem ?




tempS with printf line 784

  while (_uth->buf->context.mxcsr == 11);
    12a0:	49 8b 94 24 70 10 00 	mov    0x1070(%r12),%rdx
    12a7:	00 
    12a8:	48 83 fa 0b          	cmp    $0xb,%rdx
    12ac:	74 f2                	je     12a0 <mpthread1+0x130>
  Context *t = &(_uth->buf->context);
    12ae:	49 81 c4 00 10 00 00 	add    $0x1000,%r12
    12b5:	48 8d 35 98 08 00 00 	lea    0x898(%rip),%rsi        # 1b54 <_IO_stdin_used+0xf4>
  Context *s = &(_uth->sched_thread->context);
    12bc:	48 8d 98 00 10 00 00 	lea    0x1000(%rax),%rbx
    12c3:	4c 89 e2             	mov    %r12,%rdx
    12c6:	bf 01 00 00 00       	mov    $0x1,%edi
    12cb:	31 c0                	xor    %eax,%eax
  while (!_uth->sp_exit_check){
    12cd:	49 c1 e6 09          	shl    $0x9,%r14
    12d1:	e8 9a f6 ff ff       	callq  970 <__printf_chk@plt>
    12d6:	4a 8d 54 35 00       	lea    0x0(%rbp,%r14,1),%rdx
    12db:	eb 1c                	jmp    12f9 <mpthread1+0x189>
    12dd:	0f 1f 00             	nopl   (%rax)
      if ( _uth->buf->context.mxcsr == _uth->expVal){

tempS without printf

  while (_uth->buf->context.mxcsr == 11);
    12a0:	48 8b 91 70 10 00 00 	mov    0x1070(%rcx),%rdx
    12a7:	48 83 fa 0b          	cmp    $0xb,%rdx
    12ab:	74 f3                	je     12a0 <mpthread1+0x130>
  while (!_uth->sp_exit_check){
    12ad:	49 c1 e5 09          	shl    $0x9,%r13
  Context *s = &(_uth->sched_thread->context);
    12b1:	48 8d 98 00 10 00 00 	lea    0x1000(%rax),%rbx
  Context *t = &(_uth->buf->context);
    12b8:	48 8d 81 00 10 00 00 	lea    0x1000(%rcx),%rax
  while (!_uth->sp_exit_check){
    12bf:	4a 8d 54 2d 00       	lea    0x0(%rbp,%r13,1),%rdx
    12c4:	eb 23                	jmp    12e9 <mpthread1+0x179>
    12c6:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
    12cd:	00 00 00 
      if ( _uth->buf->context.mxcsr == _uth->expVal){





-> Using malloc

Two pointers instead of pointer array

Sometimes run fine sometimes run with seg fault
When using for and gdb gave following error:


home/wazam2/FebWeek1/program26: relocation error: /lib/x86_64-linux-gnu/libpthread.so.0: symbol __libc_thread_freeres version GLIBC_PRIVATE not defined in file libc.so.6 with link time reference


Thread 3 "program26" received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7ffff6fc3700 (LWP 51122)]
strcmp () at ../sysdeps/x86_64/strcmp.S:174
174	../sysdeps/x86_64/strcmp.S: No such file or directory.

Thread 2 "program26" received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7ffff77c4700 (LWP 51136)]
do_lookup_x (undef_name=undef_name@entry=0x7ffff7bb911b "__madvise", new_hash=140737354028272, 
    new_hash@entry=2296172972, old_hash=old_hash@entry=0x7ffff6fc2400, ref=0x7ffff7bb7590, 
    result=result@entry=0x7ffff6fc2410, scope=<optimized out>, i=<optimized out>, 
    version=0x7ffff7fe5cc0, flags=5, skip=0x0, type_class=1, undef_map=0x7ffff7fe5000)
    at dl-lookup.c:503
503	dl-lookup.c: No such file or directory.


bt

#0  strcmp () at ../sysdeps/x86_64/strcmp.S:174
#1  0x00007ffff7de67a4 in _dl_name_match_p (name=0x7ffff7bba2e1 "libc.so.6", map=<optimized out>)
    at dl-misc.c:289
#2  0x00007ffff7ddffaf in check_match (num_versions=<synthetic pointer>, 
    versioned_sym=<synthetic pointer>, map=0x7ffff7fe54f0, strtab=0x7ffff77d69d0 "", 
    symidx=35710309, sym=0x0, type_class=1, flags=5, version=0x7ffff7fe5cc0, ref=0x7ffff7bb7590, 
    undef_name=0x7ffff7bb911b "__madvise") at dl-lookup.c:121
#3  do_lookup_x (undef_name=undef_name@entry=0x7ffff7bb911b "__madvise", 
    new_hash=140737354028272, new_hash@entry=2296172972, old_hash=old_hash@entry=0x7ffff6fc2400, 
    ref=0x7ffff7bb7590, result=result@entry=0x7ffff6fc2410, scope=<optimized out>, 
    i=<optimized out>, version=0x7ffff7fe5cc0, flags=5, skip=0x0, type_class=1, 
    undef_map=0x7ffff7fe5000) at dl-lookup.c:406
#4  0x00007ffff7de023f in _dl_lookup_symbol_x (undef_name=0x7ffff7bb911b "__madvise", 
    undef_map=0x7ffff7fe5000, ref=ref@entry=0x7ffff6fc24a8, symbol_scope=0x7ffff7fe5358, 
    version=0x7ffff7fe5cc0, type_class=type_class@entry=1, flags=5, skip_map=<optimized out>)
    at dl-lookup.c:813
#5  0x00007ffff7de4f13 in _dl_fixup (l=<optimized out>, reloc_arg=<optimized out>)
    at ../elf/dl-runtime.c:112
#6  0x00007ffff7dec81a in _dl_runtime_resolve_xsavec () at ../sysdeps/x86_64/dl-trampoline.h:125
#7  0x00007ffff7bbd8b9 in advise_stack_range (guardsize=<optimized out>, pd=140737337112320, 
    size=<optimized out>, mem=0x7ffff67c3000) at allocatestack.c:386
#8  start_thread (arg=0x7ffff6fc3700) at pthread_create.c:552
#9  0x00007ffff78e6a3f in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95



-> Corrected the above bug

Performance Mean:593.1  SD: 26.319


-> mutltiple varaible working correctly


 

 ######################################################

Register losing the values is the problem

Breakpoint and all the register contents before and after the move 


Combining 15-16 local variables exp and the above experiment of checking the registers in gdb


Check for corruption of registers

Focus on register holding i

Corruption may means lack of overwriting



#####################################################################


Without printf:


callq  0x555555554920 <puts@plt>                                │
B+>
movslq %fs:0xfffffffffffffffc,%rdx                              │
add    $0x1,%ebp                                                │
shl    $0x9,%rdx                                                │
add    %r12,%rdx                                                │
mov    0xc0(%rdx),%rax                                          │
lea    0x1000(%rax),%rbx                                        │
mov    0x80(%rdx),%rax                                          │
add    $0x1000,%rax                                             │
cmpb   $0x0,0x1c0(%rdx)                                         │
jne    0x555555555060 <pfc+736>                                 │
movb   $0x1,0x1c0(%rdx)                                         │
mov    %rsp,(%rax)                                              │
lea    0x75(%rip),%rcx
mov    %rcx,0x8(%rax) 


With printf:

movslq %fs:0xfffffffffffffffc,%rdx                             │
add    $0x1,%ebp                                               │
shl    $0x9,%rdx                                               │
add    %r12,%rdx                                               │
mov    0xc0(%rdx),%rax                                         │
lea    0x1000(%rax),%rbx                                       │
mov    0x80(%rdx),%rax                                         │
add    $0x1000,%rax                                            │
cmpb   $0x0,0x1c0(%rdx)                                        │
jne    0x555555555060 <pfc+736>                                │
movb   $0x1,0x1c0(%rdx)                                        │
mov    %rsp,(%rax)                                             │
lea    0x75(%rip),%rcx
mov    %rcx,0x8(%rax)  



########################################


Hybrid Flat branch

#########################################


With replacing t
Mean:779.6  SD: 183.445

with Scheduler.c and printf
Mean:742.6  SD: 81.1692
Mean:699.2  SD: 104.033
Mean:706.2  SD: 115.505




############################################

Look at how mutex is implemented in rust

Look at the previous recording for rust crate thing 


##############################################


Your job is to migrate a fiber from one thread to another
you want to take the trust_eval code make a new crate just like trust_eval
,a crate is a package in rust,
that just spins up some fibers on couple of different threads and prints stuff 
out that will be a good start

I hope you can get that running by the next week
because basically you gonna be cutting down what already is in trust_eval


Once you have that running 


############################################################33


Do a git pull to make sure you are on the up to date master branch. It was recently updated.
Then take a look at green yield_actual, to see how fiber switching happens in the rust setup.


You could look at green.rs:spawn to see how a new fiber is started. We don’t have a way to move a fiber yet, but if we use conventional rust, it should be a sort of combo of yield and spawn.

#########

Good post on threads in rust
https://www.codegram.com/blog/learning-rust-threads/


-> ufiber performance:
Mean:604  SD: 65.0876

With hardcode 5 million in Sched
Without printf:
Mean:740.4  SD: 69.3097 

With printf:
Mean:651.7  SD: 99.3751
