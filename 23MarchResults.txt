
#######################		Fiber	##############################################

		cycles	st-fend	st-bend		instructions	branches	b-misses	
		(B)		(B)		(B)				(B)

0,32	3.7		2.8		2.1			0.8(0.2/3.6)	98M			20M			
0,1		12.8	10.1	8.3			1.5(0.1/6.4)	196M		21M
0,8		48		41		37			3.5(0.07/12)	440M		26M



-		c-ref 	c-misses 	l1-load-misses	l1-store-misses		l2 misses


0-32	68T		19T				91T				32T				1.3T	
0,1 	131M	27T				131M			615T			8.9T
0,8		77M		77M				78M				6M				62M



#######################		Delegation	##########################################

		cycles	st-fend	st-bend		instructions	branches	b-misses	
		(B)		(B)		(B)				(B)

0,32	5.6		4		3			2.8(0.5/1.4)	861M		5M						
0,1		12		4.6		4			18(1.5/0.25)	6B			14M
0,8		30		21		20			23(0.8/0.9)		8B			7M



-		c-ref 	c-misses 	l1-load-misses	l1-store-misses		l2 misses


0-32	53T		18T				92T				33T				2.2T
0,1 	60M		29T				79M				27T				3.9T	
0,8		54M		46M				59M				544T			35M


###################################################################################
Change of passing context instead of pointer to the thread 

	Fiber
HT	118,18
SS	392,25
DS  1700s or 2500s (prev 3000s)
2S


################# 27 March ###########################################
######## Fiber after changing position of rep nops ###################

		Fiber
HT		109,4
SS		427,33
DS		2508,344 (1900's for 10x cycles)  ------ 2077, 309 (10 seconds apart)
25		2618, 49								 2505,141  (10 seconds apart) 


DS (10 seconds sleep)
Execution time is 1900 clock cycles
Execution time is 1914 clock cycles
Execution time is 1882 clock cycles
Execution time is 2671 clock cycles
Execution time is 2699 clock cycles
Execution time is 2005 clock cycles
Execution time is 1854 clock cycles
Execution time is 1940 clock cycles
Execution time is 1860 clock cycles
Execution time is 2045 clock cycles





		cycles	st-fend	st-bend		instructions	branches	b-misses	
		(B)		(B)		(B)				(B)

HT
SS
DS



-		c-ref 	c-misses 	l1-load-misses	l1-store-misses		l2 misses


HT
SS
DS


// capture actual temperature, run perf with multiple run option
	
	sensors command
	program19_exp run on 0, 8 
	temperature of the Socket 0: 41 to 45 
	temperature of the Socket 1: 40 to 43
	temperature of the core 0:   39 to 45
	temperature of the core 8: 	 35 to 40


// Figure out L1-dcache-loads 

	nothing useful could be found

// try removing sp_exit_check from fibers

		Fiber
	HT	104, 4
	SS	425, 25
	DS 	2292, 314
	2S

	Execution time is 1916 clock cycles
	Execution time is 2549 clock cycles
	Execution time is 2597 clock cycles
	Execution time is 2534 clock cycles
	Execution time is 2564 clock cycles
	Execution time is 2554 clock cycles
	Execution time is 1909 clock cycles
	Execution time is 2487 clock cycles
	Execution time is 1929 clock cycles
	Execution time is 1884 clock cycles

	L1-Dcache-loads increased from 21B to 24B


// Hardcoding in while loop (came up on my own)

		Fiber
	HT	105, 5
	SS	385, 15
	DS  1871, 120 (one 2219 others smaller)
	2S  2215, 52

	most things are still nearly similar including L1-Dcache-loads 23B

	//using id for changing the bit

		Fiber
	HT  103, 0.9
	SS	387, 26
	DS  1817, 43
	2S




// introduce 1 rep nop in case of else (fiber)

	SS 440, 27
	DS 2488, 424 (1900s or 2900s)



// Introduce 1 rep nop in while loop of delegaton (delegation)

		Delegation (before with 10s)
 	HT	188, 7
	SS	429, 24
	DS  1322, 97
	2S 

		Delegation (after with 10s)
 	HT  187, 5
	SS 	416, 25
	DS 	1305, 57
	2S 	1424, 36


// LLC store misses, dig deep (why 1.5 million difference)

	difference close down

* read it again carefully line by line
// L1-dcache-store-misses, why not the same?

	can't figure out much

// Check for fiber schedular loop, try to bring loads to 1 instead of 4

// check 10 s delay for delegation too



 cpu-cycles
 instructions
 cache-references
 cache-misses
 branch-instructions
 branch-misses
 bus-cycles

 cpu-clock
 task-clock
 page-faults
 minor-faults
 major-faults
 context-switches
 cpu-migrations
 alignment-faults
 emulation-faults

 L1-dcache-loads
 L1-dcache-load-misses
 L1-dcache-stores
 L1-dcache-store-misses
 L1-dcache-prefetches
 L1-dcache-prefetch-misses
 L1-icache-loads
 L1-icache-load-misses
 L1-icache-prefetches
 L1-icache-prefetch-misses
 LLC-loads
 LLC-load-misses
 LLC-stores
 LLC-store-misses

 LLC-prefetch-misses
 dTLB-loads
 dTLB-load-misses
 dTLB-stores
 dTLB-store-misses
 dTLB-prefetches
 dTLB-prefetch-misses
 iTLB-loads
 iTLB-load-misses
 branch-loads
 branch-load-misses                




 cpu-cycles,instructions,cache-references,cache-misses,branch-instructions,branch-misses,bus-cycles

 cpu-clock,task-clock,page-faults,minor-faults,major-faults,context-switches,cpu-migrations,alignment-faults,emulation-faults

 L1-dcache-loads,L1-dcache-load-misses,L1-dcache-stores,L1-dcache-store-misses,L1-dcache-prefetches,L1-dcache-prefetch-misses,L1-icache-loads,L1-icache-load-misses,L1-icache-prefetches,L1-icache-prefetch-misses,LLC-loads,LLC-load-misses,LLC-stores,LLC-store-misses

 LLC-prefetch-misses,dTLB-loads,dTLB-load-misses,dTLB-stores,dTLB-store-misses,dTLB-prefetches,dTLB-prefetch-misses,iTLB-loads,iTLB-load-misses,branch-loads,branch-load-misses




#############################################################3
8,16 Fibers


Execution time is 1949 clock cycles
globalVariable: 10000000
global_var1: 0
global_var2: 0

 Performance counter stats for './program19_exp':

    43,051,945,367      cpu-cycles                #    2.429 GHz                      (19.19%)
    42,136,545,168      instructions              #    0.98  insn per cycle                                              (23.07%)
        77,151,354      cache-references          #    4.352 M/sec                    (23.12%)
        73,142,085      cache-misses              #   94.803 % of all cache refs      (23.16%)
    12,525,460,174      branch-instructions       #  706.579 M/sec                    (23.21%)
        27,433,760      branch-misses             #    0.22% of all branches          (23.25%)
     1,770,076,959      bus-cycles                #   99.853 M/sec                    (15.51%)
      17726.909612      cpu-clock (msec)          #    2.000 CPUs utilized          
      17726.911334      task-clock (msec)         #    2.000 CPUs utilized          
               119      page-faults               #    0.007 K/sec                  
               119      minor-faults              #    0.007 K/sec                  
                 0      major-faults              #    0.000 K/sec                  
                21      context-switches          #    0.001 K/sec                  
                 2      cpu-migrations            #    0.000 K/sec                  
                 0      alignment-faults          #    0.000 K/sec                  
                 0      emulation-faults          #    0.000 K/sec                  
    21,026,969,546      L1-dcache-loads           # 1186.161 M/sec                    (15.28%)
        79,228,286      L1-dcache-load-misses     #    0.38% of all L1-dcache hits    (7.67%)
       184,631,133      L1-dcache-stores          #   10.415 M/sec                    (7.67%)
         3,909,223      L1-dcache-store-misses    #    0.221 M/sec                    (7.67%)
   <not supported>      L1-dcache-prefetches                                        
        32,949,846      L1-dcache-prefetch-misses #    1.859 M/sec                    (7.67%)
   <not supported>      L1-icache-loads                                             
           457,553      L1-icache-load-misses                                         (11.51%)
   <not supported>      L1-icache-prefetches                                        
   <not supported>      L1-icache-prefetch-misses                                   
        66,641,324      LLC-loads                 #    3.759 M/sec                    (15.34%)
        62,735,012      LLC-load-misses           #   94.14% of all LL-cache hits     (15.34%)
        10,628,073      LLC-stores                #    0.600 M/sec                    (7.67%)
        10,631,235      LLC-store-misses          #    0.600 M/sec                    (7.67%)
         5,104,920      LLC-prefetch-misses       #    0.288 M/sec                    (7.67%)
    21,013,323,771      dTLB-loads                # 1185.391 M/sec                    (7.67%)
        20,087,601      dTLB-load-misses          #    0.10% of all dTLB cache hits   (7.67%)
       184,755,107      dTLB-stores               #   10.422 M/sec                    (7.67%)
            11,484      dTLB-store-misses         #    0.648 K/sec                    (7.67%)
   <not supported>      dTLB-prefetches                                             
   <not supported>      dTLB-prefetch-misses                                        
             1,317      iTLB-loads                #    0.074 K/sec                    (7.67%)
             3,545      iTLB-load-misses          #  269.17% of all iTLB cache hits   (11.51%)
    12,520,296,262      branch-loads              #  706.288 M/sec                    (15.34%)
        27,372,591      branch-load-misses        #    1.544 M/sec                    (15.34%)

       8.865149283 seconds time elapsed




8, 16 Delegation

Execution time is 1293 clock cycles
globalCounter 10000000 

 Performance counter stats for './delg':

    27,128,619,168      cpu-cycles                #    2.306 GHz                      (19.04%)
    16,958,572,727      instructions              #    0.63  insn per cycle                                              (22.91%)
        54,861,199      cache-references          #    4.664 M/sec                    (22.98%)
        45,752,915      cache-misses              #   83.398 % of all cache refs      (23.05%)
     5,567,437,269      branch-instructions       #  473.288 M/sec                    (23.12%)
         7,745,862      branch-misses             #    0.14% of all branches          (23.19%)
     1,174,521,853      bus-cycles                #   99.846 M/sec                    (15.51%)
      11763.305752      cpu-clock (msec)          #    1.999 CPUs utilized          
      11763.309648      task-clock (msec)        3,909,223      L1-dcache-store-misses    #    0.221 M/sec 3,909,223      L1-dcache-store-misses    #    0.221 M/sec  #    1.999 CPUs utilized          
                88      page-faults               #    0.007 K/sec                  
                88      minor-faults              #    0.007 K/sec                  
                 0      major-faults              #    0.000 K/sec                  
                14      context-switches          #    0.001 K/sec                  
                 3      cpu-migrations            #    0.000 K/sec                  
                 0      alignment-faults          #    0.000 K/sec                  
                 0      emulation-faults          #    0.000 K/sec                  
     5,592,904,108      L1-dcache-loads           #  475.453 M/sec                    (15.23%)
        59,171,773      L1-dcache-load-misses     #    1.06% of all L1-dcache hits    (7.75%)
        52,977,029      L1-dcache-stores          #    4.504 M/sec                    (7.75%)
           140,822      L1-dcache-store-misses    #    0.012 M/sec                    (7.75%)
   <not supported>      L1-dcache-prefetches                                        
        42,906,438      L1-dcache-prefetch-misses #    3.647 M/sec                    (7.75%)
   <not supported>      L1-icache-loads                                             
           282,661      L1-icache-load-misses                                         (11.63%)
   <not supported>      L1-icache-prefetches                                        
   <not supported>      L1-icache-prefetch-misses                                   
        42,827,790      LLC-loads                 #    3.641 M/sec                    (15.51%)
        33,820,826      LLC-load-misses           #   78.97% of all LL-cache hits     (15.51%)
        11,935,321      LLC-stores                #    1.015 M/sec                    (7.72%)
        11,919,440      LLC-store-misses          #    1.013 M/sec                    (7.65%)
         2,331,252      LLC-prefetch-misses       #    0.198 M/sec                    (7.62%)
     5,592,935,562      dTLB-loads                #  475.456 M/sec                    (7.62%)
            36,241      dTLB-load-misses          #    0.00% of all dTLB cache hits   (7.62%)
        53,048,077      dTLB-stores               #    4.510 M/sec                    (7.62%)
             5,934      dTLB-store-misses         #    0.504 K/sec                    (7.62%)
   <not supported>      dTLB-prefetches                                             
   <not supported>      dTLB-prefetch-misses                                        
               460      iTLB-loads                #    0.039 K/sec                    (7.62%)
               228      iTLB-load-misses          #   49.57% of all iTLB cache hits   (11.42%)
     5,561,456,656      branch-loads              #  472.780 M/sec                    (15.23%)
         7,526,395      branch-load-misses        #    0.640 M/sec                    (15.23%)

       5.885141760 seconds time elapsed







##############################################
April 14 Meeting Shifted to April 21
##############################################

* (Run both with 10 seconds pause and without it. Also brainstorm other ideas causing the 
slow down. Google PowerState)

	Without 10 sec
	sensors command
	program20_exp run on 0, 8 
	temperature of the Socket 0: 42 to 45 
	temperature of the Socket 1: 42 to 46
	temperature of the core 0:   40 to 45
	temperature of the core 8: 	 36 to 43


	With 10 sec
	sensors command
	program20_exp run on 0, 8 
	temperature of the Socket 0: 42 to 44 
	temperature of the Socket 1: 42 to 43
	temperature of the core 0:   39 to 44
	temperature of the core 8: 	 37 to 41

	In first runs the temperature didn't increased that much


	Execution time is 2827 clock cycles
	Execution time is 2499 clock cycles
	Execution time is 1947 clock cycles
	Execution time is 2248 clock cycles
	Execution time is 1932 clock cycles
	Execution time is 2440 clock cycles
	Execution time is 2800 clock cycles
	Execution time is 1858 clock cycles
	Execution time is 2425 clock cycles
	Execution time is 1887 clock cycles




* Send email for Sublime Subscription
* Check again for frequency scaling

Run on 0, 8

1 run not bound:
	 2.360 GHz (1948 cycles) 
	 2.584     (1893)
	 2.432		(2947)
	 2.552		(1924)

	 num of cycles and instruction got increased

10 run not bound:
	Either 1900s or 2900s

	Freq: 2.463 GHz (+- 7.70 %)

	2970
	1895
	2909
	1994
	1913
	2861
	1928
	2996
	1890
	2953

10 run Bound to 0:
	Freq 2.483 GHz (+- 6.42)

10 run Bound to 8:
	Freq 2.438 Ghz (+- 7.83)


* Try the experiment on other machines as well
	10s gap

	Lines: (Not sure about the architechture)

	HT(0, 32)	2005, 94		
	SS(0,1) 	251, 8.95
	DS(0,8)		1809, 119
	DS(0,8)		1859, 79  (Without 10s)

	Nodes: 

	HT(0, 32)	1724, 85	(Actuall 2 Hops away)
	SS(0,1)		539, 46
	DS(0,8)		493, 22
	DS(0,8)

	HT(0, 64)	89.4, 2.0
	DS(0, 16)	1334, 174





* Measure the other temps like Memory or Memory controller or Bus temp (idk whether there 
are sensors there). Search for off core temperature sensors. 

-- Not much sesnors abroad



###############################################

# Repeat the experiment on Quads as well (1 Machine out of the 4 is enough)
# Repeat the other experiments (delegation/locks) on Nodes and Quads as well

# Any user fiber library (e.g libfiber start with that) check for performance of fiber migrations

# Explore Rust (write some small programs and try doing the delegation in rust(not really)), Noman and Ben are deep in Rust 

-> Picking up Rust isn't that easy. Easily a month to pickup, Syntax isn't a big deal but Model/core-concepts is: borrow-checker, reference lifetime

-> Talk to Ben and Noman about how they feel about Rust


###################################################
April 27


# Repeat the experiment on Quads as well (1 Machine out of the 4 is enough)
# Repeat the other experiments (delegation/locks) on Nodes and Quads as well

Experiments with 10s wait


Nodes: 

		Fiber (From Past week)
	HT(0, 64)	89.4, 2.0
	SS(0,1)		539, 46
	DS(0, 16)	1334, 174
	2S(0, 32)	1724, 85


		Delegation
	HT(0, 64)	150, 0.4
	SS(0,1) 	463, 39
	DS(0,16)	986, 65
	2S(0, 32)   1151, 124

		Locks (Spin)
	HT(0,64)	58, 4
	SS(0,1) 	172, 36
	DS(0,16)	354, 37
	2S(0, 32)	471, 40

		Locks (Mutex)
	HT(0,64)	70, 0	
	SS(0,1)		313, 17
	DS(0,16)	316, 22
	2S(0,32)	321, 31



54, 2 

Quads 1: 

		Fiber
	HT(0,28)	587, 1.3  (Repeated Twice, Similar Result)	  
	SS(0,1)		747, 8.1 
	DS(0,14)	1148, 88
	2S

		Delegation
	HT(0,28)	1120, 1.1
	SS(0,1)		1500, 35 	
	DS(0,14)    2042, 84
	2S

		Locks (Spin)
	HT(0,28)	508, 1
	SS(0,1)     571, 3.6
	DS(0,14)	695, 22
	2S

		Locks (Mutex)
	HT(0,28)	527, 0.5
	SS(0,1) 	698, 20
	DS(0,14)	802, 28
	2S



# Any user fiber library (e.g libfiber start with that) check for performance of fiber migrations

-> Boost: A fiber c


###########################################################

# Investigate by changing repnops for quads experiment
# Try passing the core number as a parameter
# Try ticket locks for preventing reacusiation / and measure how often re-acquisition occurs
-> add another counter to check for re-acquisitions 

# With and without rep nops (fiber and delegation) 

get-opt (function for command line arguments) there are other as well -- google command line 
params in c

#Use Makefile (compile time arguments -D, by the use of #ifdef change whethere to use repnops or not)


########################################################



------- Quad 3 Experiment -------
Without Sleep
Without Repnops

Fiber
HT(0, 28) Mean:111.8  SD: 0.4
SS(0, 1)  Mean:719.5  SD: 19.9261
DS(0, 14) Mean:1541  SD: 69.0912
---------------------------------------
delegation_exp.c
HT(0, 28) Mean:86.3  SD: 0.781025
SS(0, 1)  Mean:471  SD: 20.283
DS(0, 14) Mean:548.1  SD: 12.5096
----------------------------------------
locks_exp.c with spinlocks
HT(0, 28) Mean:24  SD: 0
SS(0, 1)  Mean:102.2  SD: 4.21426
DS(0, 14) Mean:281.5  SD: 29.588

######################################

With Repnops

Fiber
HT(0, 28) Mean:536.8  SD: 1.6
SS(0, 1)  Mean:698.1  SD: 7.51598
DS(0, 14) Mean:1146  SD: 53.4359
---------------------------------------
delegation_exp.c
HT(0, 28) Mean:1120.7  SD: 2.49199
SS(0, 1)  Mean:1465  SD: 37.2586
DS(0, 14) Mean:1995.6  SD: 107.72
----------------------------------------
locks_exp.c with spinlocks
HT(0, 28) Mean:507.1  SD: 0.830662
SS(0, 1)  Mean:571.8  SD: 2.44131
DS(0, 14) Mean:686.5  SD: 26.5603

########################################################

Special Counter for spinlocks re-aquisition

Without Repnops 
locks_exp2.c with spinlocks

HT(0, 28) 4672794
SS(0, 1)  4850267
DS(0, 14) 4475495

With Repnops

HT(0, 28) 2087
SS(0, 1)  1902
DS(0, 14) 53953








#######################################################
(Meeting Saturday May 16, For Next Meeting on May 19)
# ticket locks (I can a)
# Add to your script a gnu plot which makes a bar chart
set the file to the bar chart
#Repeat the experiment on other machine (e.g. nodes)

#####################################################
Summer goals

Green sleep branch in rust branches link shared by the 

Berrel Fish Operating System EPFL
######################################################



------- Nodes Experiment -------
Without Sleep
Without Repnops

program20_exp.c 

HT(0, 64) Mean:108.5  SD: 3.23265
SS(0, 1)  Mean:562.9  SD: 28.6965
DS(0, 16) Mean:1742.5  SD: 198.638
---------------------------------------
delegation_exp.c

HT(0, 64) Mean:87.9  SD: 0.538516
SS(0, 1)  Mean:448.7  SD: 54.2937
DS(0, 16) Mean:770.7  SD: 128.322
----------------------------------------
locks_exp.c with spinlocks

HT(0, 64) Mean:41.7  SD: 1.48661
SS(0, 1)  Mean:71.8  SD: 11.4
DS(0, 16) Mean:78.1  SD: 9.37497

######################################
With Repnops

program20_exp.c 

HT(0, 64) Mean:89.1  SD: 3.83275
SS(0, 1)  Mean:542  SD: 37.9895
DS(0, 16) Mean:1618.1  SD: 155.775
---------------------------------------
delegation_exp.c

HT(0, 64) Mean:173.9  SD: 13.5311
SS(0, 1)  Mean:435.7  SD: 38.6783
DS(0, 16) Mean:938.3  SD: 140.526
----------------------------------------
locks_exp.c with spinlocks

HT(0, 64) Mean:57.1  SD: 0.3
SS(0, 1)  Mean:191.2  SD: 20.6485
DS(0, 16) Mean:354.7  SD: 23.4011


###############################################

Time with Ticket Locks

------- Nodes Experiment -------
Without Sleep
----------------------------------------
Without repnops
locks_exp2.c with ticketLocks

HT(0, 64) Mean:66  SD: 4.64758
SS(0, 1)  Mean:198  SD: 27.6659
DS(0, 16) Mean:367.1  SD: 43.3208
----------------------------------------
With repnops
locks_exp2.c with ticketLocks

HT(0, 64) Mean:57  SD: 0
SS(0, 1)  Mean:192.5  SD: 17.8059
DS(0, 16) Mean:330.1  SD: 39.5056


############################################


Special Counter for ticketlocks re-aquisition

Without Repnops 
locks_exp2.c with ticketlocks

HT(0, 64) 150-2000
SS(0, 1)  300-1000
DS(0, 16) 90000-1475000

With Repnops

HT(0, 64) 800-2000
SS(0, 1)  2000-7000
DS(0, 16) 1568000-3436000

##########################################
Confirmed TicketLock

------- Nodes Experiment -------
Without Sleep
----------------------------------------
Without repnops
locks_exp2.c with ticketlocks


HT(0, 64) Mean:73.3  SD: 0.458258
SS(0, 1)  Mean:215.2  SD: 32.4586
DS(0, 16) Mean:233.3  SD: 82.0074
----------------------------------------
With repnops
locks_exp2.c with ticketlocks

HT(0, 64) Mean:66.1  SD: 4.52659
SS(0, 1)  Mean:277.3  SD: 35.2308
DS(0, 16) Mean:333.1  SD: 101.206


##########################################

Bar chart ???

HT, SS, 


##################################

Nodes will be busy, try quads

# Start using bar chart

#Measure where we are spending time, use perf c2c (not time, cache misses) and with perf record
-> Investigage for the round trip latency for fibers

# Lookup __rdtsc (compiler intrinsic) for measuring time instead of what I am already doing

# Move thread pinning in the threads
# Measure time after the entering and leaving the barrier

# Rust repository brach: green_sleep 
Make a new test case fiber migration (most test cases will be in lib.rs)
normal delegation with a function calling a function
wrapping the varialbe in trust

-> End goal: migrate the function to the server and the function keep on running

-> Print migrate -> Check which client -> Check which fiber -> Add that to the run 


#################################


Nodes will be busy, try quads

# Start using bar chart (Done)

#Measure where we are spending time, use perf c2c (not time, cache misses) and with perf record
-> Investigage for the round trip latency for fibers (Done)


c2c HT Fiber
17 cpu/mem-loads, ldlat=30/P (Cannot decipher)
4k cpu/mems-stores/P
	44 pfc -> 86% mov ecx, 0x38(%rax)
	42 pfc -> add to global variable
	13 Context Switch


c2c SS Fiber
23k cpu/mem-loads, ldlat=30/P
	86 Schedular -> 100% mov ecx, 0x38(%rax)
	7  contextSwitch
28k cpu/mems-stores/P
	53 Schedular -> call to context Switch
	40 pfc -> Additon to global variable


c2c DS Fiber
55k cpu/mem-loads, ldlat=30/P
	90 Schedular -> 100% mov ecx, 0x38(%rax)
	9  contextSwtich
62k cpu/mems-stores/P
	49 Scheudlar -> call to context Switch
	38 pfc -> Additon to global variable



# Lookup __rdtsc (compiler intrinsic) for measuring time instead of what I am already doing

# Move thread pinning in the threads
# Measure time after the entering and leaving the barrier

# Rust repository brach: green_sleep 
Make a new test case fiber migration (most test cases will be in lib.rs)
normal delegation with a function calling a function
wrapping the varialbe in trust

-> End goal: migrate the function to the server and the function keep on running

-> Print migrate -> Check which client -> Check which fiber -> Add that to the run 